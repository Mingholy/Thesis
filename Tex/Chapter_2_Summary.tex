\chapter{传统的基于统计的命名实体识别方法综述}
本章从语言模型入手，介绍了当前常见的几种基于统计的命名实体识别方法，包括以马尔可夫模型为基础的隐马尔科夫模型、结合了最大熵模型的最大熵马尔可夫模型，将命名实体识别任务转化为文本分类问题的支持向量机与决策树方法，以及目前工业界最常用，并且具有很好效果的条件随机场方法。
最后，我们通过比较这几种方法的原理和在命名实体识别任务上的性能，分析了传统的基于统计的命名实体识别方法的局限性；
同时，针对二元语言模型、隐马尔可夫模型，探讨了它们对基于神经网络的方法存在的潜在改进思路；
最后，针对条件随机场模型，探讨了其与神经网络相结合的原理和意义。
\section{隐马尔可夫模型与最大熵马尔可夫模型}
隐马尔可夫模型（hidden Markov model, HMM)和结合了最大熵方法的最大熵马尔可夫模型（maximum-entropy Markov model, MEMM）在解决序列标注问题得到了被广泛应用，其中隐马尔可夫模型还在语音识别、统计机器翻译上有着重要的应用。
\subsection{预备知识}
在介绍上述两种模型之前，需要先对n元语言模型和一般的马尔可夫模型做简要的介绍。
语言模型是一种构建简单、直接的模型，是人们对语言这一抽象概念的一种相对简单的数学形式化。
语言模型实际上是针对特定语料集的统计特征建立的模型。
一般来说，对于给定的文本序列
\begin{equation}
    S = w_1 w_2 w_3 \dots w_n
\end{equation}
在语言模型下，该文本序列可判定为句子的概率$P(S)$即为模型的输出，其中$w_i, i\in \{1,2,\dots,n\}$称为文本序列的基本元素，基本元素可以是字，典型的如中文、日文等；也可以是词，如英文、法文等西方语言；也可以是其他人为规定的文本元素。
概率$P(S)$可由下式计算：
\begin{align}
    \label{eq: language model} P(S) &= P(w_1)P(w_2 | w_1)P(w_3 | w_1 w_2)\cdots P(w_n | w_1 w_2 \cdots w_{n-1})\\
    &= \prod_{i=1}^{n}P(w_i | w_1 \cdots w_{i-1})
\end{align}
可以看出，产生第$i$个基本元素的概率依赖前面所有的基本元素，则随着句子长度的增加，需要估计的参数就会呈指数级增加。这种规模的参数很难从有限的训练样本中准确估计，其计算量所需要的时间成本也难以接受。

在式2.2的基础上，若假设产生第$n$个基本元素的概率不再依赖前面的所有基本元素，而只依赖它之前的$n-1$个，则称该语言模型为n元语言模型（n-gram)。当$n=1$时，即每个基本元素的出现是独立的，不依赖其前面的基本元素，则称该模型为一元语言模型（uni-gram)；当$n=2$时，即每个基本元素的出现仅与它前面的一个元素相关，则称该模型为二元语言模型(bi-gram)，且注意到一阶离散马尔科夫链的性质：
\begin{equation}
    P\{X_{n+1} = x|X_1 = x_1, X_2 = x_2, \dots, X_n = x_n\} = P\{X_{n+1} = x| X_n = x_n\}
\end{equation}
其中$X_1, X_2, \dots , X_n$是随机状态序列，$x_1, x_2, \dots, x_n$是过程中的状态，则可知随机状态序列中$X_{n+1}$关于之前所有序列元素的条件概率分布是只关于${X_n}$的函数，即在该随机状态序列中，某时刻状态只与其前一时刻的状态有关。上述二元语言模型满足此性质，则二元语言模型是一个马尔可夫模型；当$n=3$时，第$n$个基本元素与其之前的两个元素相关，称为三元语言模型（tri-gram），显然三元语言模型不满足马尔科夫性，故它不能称为马尔可夫模型。但一般来说，对于$n\geq 3$的语言模型来说，其状态空间可描述为多个二元状态的乘积，则此时可将多元语言模型转化为马尔可夫模型，$n$元语言模型依赖前面$n-1$个元素，称为$n-1$阶马尔可夫模型。

以二元语言模型为例，假定以词为文本序列的基本元素，我们可以将它依据训练语料进行文本序列概率预测的过程理解为两个步骤：首先，使用训练语料建立词-词的转移矩阵，即两个词相邻，则存在一次前词到后词的转移；随后，计算待预测序列中所有存在的2-gram的概率，即估计
\begin{equation}
    P(w_i|w_{i-1}) = \frac{c(w_{i-1}w_i)}{\sum_{w_i}c(w_{i-1}w_i)}
\end{equation}
其中$w_i$是词，$c(w_{i-1}w_i)$是2-gram$w_{i-1}w_i$在训练预料中的频数。最后计算序列条件概率：
\begin{equation}
    p(s) = \prod_{i=1}^{l+1}p(w_i|w^{i-1}_{i-n+1})
\end{equation}
其中$w^j_i$表示词序列$w_i,\dots,w_j$，且约定元素$w_{-n+2}$到$w_0$为统一的序列起始元素<BOS>，$w_{l+1}$为统一的序列结束元素<EOS>。

对于n元语言模型，存在着两方面的问题。我们以中文和2-gram为例进行讨论。
首先不论是以词为单位还是以字为单位统计2-gram，由于语料总是有限的，无论规模多么庞大的语料总不可能会覆盖每两个词的组合，即一定会有部分2-gram$w_p w_q$其经过统计计算得到的$p(w_p|w_q)=0$，于是所有以该2-gram为子序列的文本序列（句子）在该语言模型下的预测概率都将是0。这显然不符合实际，因为一句话训练语料未曾出现过，并不代表它不可能出现。
实际上，这种2-gram存在的数量还很多，导致词-词转移矩阵会非常稀疏，预测的结果也会受到严重影响。这一问题称为数据平滑问题，解决这一问题基本思路是给予所有统计概率为0的2-gram以一个较小的非零概率，具体包括加一平滑、古德-图灵法等，限于篇幅，这里就不再详细讨论。

其次，进行这种模型的构建需要统计大量的词共现信息，谷歌进行的google books ngram项目中，中文的n-gram（$n\in[1, 2, \dots, 5])$数量已经达到了百亿级别，如何有效存储、检索和计算如此规模的数据，也是其应用的一个问题。
\subsection{隐马尔可夫模型}
与一般的马尔可夫模型不同的是，隐马尔可夫模型包含两个随机过程，一个是状态可观察的输出序列，称为观测状态；一个是状态不可观察的隐藏序列，称为隐藏状态。
并且，观测状态由隐藏状态决定，
\subsection{最大熵马尔可夫模型}
（介绍基本原理，选取一篇论文介绍该方法原理在实际的命名实体识别中如何应用）

\section{支持向量机与决策树}
\subsection{支持向量机}
（介绍基本原理，选取一篇论文介绍该方法原理在实际的命名实体识别中如何应用）
\subsection{决策树}
（介绍基本原理，选取一篇论文介绍该方法原理在实际的命名实体识别中如何应用）


\section{条件随机场}
（介绍基本原理，选取一篇论文介绍该方法原理在实际的命名实体识别中如何应用）
\section{基于统计语言模型方法的比较和分析}
\section{本章小结}

