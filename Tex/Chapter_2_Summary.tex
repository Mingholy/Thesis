\chapter{传统的基于统计的命名实体识别方法综述}
本章从语言模型入手，介绍了当前常见的几种基于统计的命名实体识别方法，包括以马尔可夫模型为基础的隐马尔科夫模型、结合了最大熵模型的最大熵马尔可夫模型，将命名实体识别任务转化为文本分类问题的支持向量机与决策树方法，以及目前工业界最常用，并且具有很好效果的条件随机场方法。
最后，我们通过比较这几种方法的原理和在命名实体识别任务上的性能，分析了传统的基于统计的命名实体识别方法的局限性；
同时，针对二元语言模型、隐马尔可夫模型，探讨了它们对基于神经网络的方法存在的潜在改进思路；
最后，针对条件随机场模型，探讨了其与神经网络相结合的原理和意义。
\section{隐马尔可夫模型与最大熵马尔可夫模型}
隐马尔可夫模型（hidden Markov model, HMM)和结合了最大熵方法的最大熵马尔可夫模型（maximum-entropy Markov model, MEMM）在解决序列标注问题得到了被广泛应用，其中隐马尔可夫模型还在语音识别、统计机器翻译上有着重要的应用。
\subsection{预备知识}
在介绍上述两种模型之前，需要先对n元语言模型和一般的马尔可夫模型做简要的介绍。
语言模型是一种构建简单、直接的模型，是人们对语言这一抽象概念的一种相对简单的数学形式化。
语言模型实际上是针对特定语料集的统计特征建立的模型。
一般来说，对于给定的文本序列
\begin{equation}
    S = w_1 w_2 w_3 \dots w_n
\end{equation}
在语言模型下，该文本序列可判定为句子的概率$P(S)$即为模型的输出，其中$w_i, i\in \{1,2,\dots,n\}$称为文本序列的基本元素，基本元素可以是字，典型的如中文、日文等；也可以是词，如英文、法文等西方语言；也可以是其他人为规定的文本元素。
概率$P(S)$可由下式计算：
\begin{align}
    \label{eq: language model} P(S) &= P(w_1)P(w_2 | w_1)P(w_3 | w_1 w_2)\cdots P(w_n | w_1 w_2 \cdots w_{n-1})\\
    &= \prod_{i=1}^{n}P(w_i | w_1 \cdots w_{i-1})
\end{align}
可以看出，产生第$i$个基本元素的概率依赖前面所有的基本元素，则随着句子长度的增加，需要估计的参数就会呈指数级增加。这种规模的参数很难从有限的训练样本中准确估计，其计算量所需要的时间成本也难以接受。

在式2.2的基础上，若假设产生第$n$个基本元素的概率不再依赖前面的所有基本元素，而只依赖它之前的$n-1$个，则称该语言模型为n元语言模型（n-gram)。当$n=1$时，即每个基本元素的出现是独立的，不依赖其前面的基本元素，则称该模型为一元语言模型（uni-gram)；当$n=2$时，即每个基本元素的出现仅与它前面的一个元素相关，则称该模型为二元语言模型(bi-gram)，且注意到一阶离散马尔科夫链的性质：
\begin{equation}
    P\{X_{n+1} = x|X_1 = x_1, X_2 = x_2, \dots, X_n = x_n\} = P\{X_{n+1} = x| X_n = x_n\}
\end{equation}
其中$X_1, X_2, \dots , X_n$是随机状态序列，$x_1, x_2, \dots, x_n$是过程中的状态，则可知随机状态序列中$X_{n+1}$关于之前所有序列元素的条件概率分布是只关于${X_n}$的函数，即在该随机状态序列中，某时刻状态只与其前一时刻的状态有关。上述二元语言模型满足此性质，则二元语言模型是一个马尔可夫模型；当$n=3$时，第$n$个基本元素与其之前的两个元素相关，称为三元语言模型（tri-gram），显然三元语言模型不满足马尔科夫性，故它不能称为马尔可夫模型。但一般来说，对于$n\geq 3$的语言模型来说，其状态空间可描述为多个二元状态的乘积，则此时可将多元语言模型转化为马尔可夫模型，$n$元语言模型依赖前面$n-1$个元素，称为$n-1$阶马尔可夫模型。

以二元语言模型为例，假定以词为文本序列的基本元素，我们可以将它依据训练语料进行文本序列概率预测的过程理解为两个步骤：首先，使用训练语料建立词-词的转移矩阵，即两个词相邻，则存在一次前词到后词的转移；随后，计算待预测序列中所有存在的2-gram的概率，即估计
\begin{equation}
    P(w_i|w_{i-1}) = \frac{c(w_{i-1}w_i)}{\sum_{w_i}c(w_{i-1}w_i)}
\end{equation}
其中$w_i$是词，$c(w_{i-1}w_i)$是2-gram$w_{i-1}w_i$在训练预料中的频数。最后计算序列条件概率：
\begin{equation}
    p(s) = \prod_{i=1}^{l+1}p(w_i|w^{i-1}_{i-n+1})
\end{equation}
其中$w^j_i$表示词序列$w_i,\dots,w_j$，且约定元素$w_{-n+2}$到$w_0$为统一的序列起始元素<BOS>，$w_{l+1}$为统一的序列结束元素<EOS>。

对于n元语言模型，存在着两方面的问题。我们以中文和2-gram为例进行讨论。
首先不论是以词为单位还是以字为单位统计2-gram，由于语料总是有限的，无论规模多么庞大的语料总不可能会覆盖每两个词的组合，即一定会有部分2-gram$w_p w_q$其经过统计计算得到的$p(w_p|w_q)=0$，于是所有以该2-gram为子序列的文本序列（句子）在该语言模型下的预测概率都将是0。这显然不符合实际，因为一句话训练语料未曾出现过，并不代表它不可能出现。
实际上，这种2-gram存在的数量还很多，导致词-词转移矩阵会非常稀疏，预测的结果也会受到严重影响。这一问题称为数据平滑问题，解决这一问题基本思路是给予所有统计概率为0的2-gram以一个较小的非零概率，具体包括加一平滑、古德-图灵法等，限于篇幅，这里就不再详细讨论。

其次，进行这种模型的构建需要统计大量的词共现信息，谷歌进行的google books ngram项目中，中文的n-gram（$n\in[1, 2, \dots, 5])$数量已经达到了百亿级别，如何有效存储、检索和计算如此规模的数据，也是其应用的一个问题。
\subsection{隐马尔可夫模型}
一般的马尔可夫模型，其状态和状态的转移都是可观测的。
如果可观测的结果并非是马尔可夫模型真正的状态序列，而是状态序列的经随机函数映射的结果，且状态及其转移过程都是不可见的，则这种模型称为“隐马尔可夫模型”。
我们可以通过下列方式描述一个隐马尔可夫模型：
\begin{equation}
    \mu =(S, K, \Matrix{A}, \Matrix{B}, \Matrix{\pi})
\end{equation}
其中，$S$为隐藏状态的集合，$K$为可能输出结果集合，$\Matrix{A}$、$\Matrix{B}$、$\Matrix{\pi}$分别是隐藏状态的转移概率矩阵、隐藏状态到输出结果的概率分布矩阵和隐藏状态的初始概率分布。
设$S={s_m}, 1\leq m \leq M$，即存在$M$种隐藏状态；$O={o_n}, 1\leq n\leq N$，即存$N$种可观测到的输出结果，则$\Matrix{A}$中元素$a_{ij}$满足：
\begin{align}
    a_{ij} &= P(q_t = s_j|q_{t-1} = s_i)， 1\leq i,j \leq M\\
    a_{ij} &\geq 0
    \sum^{N}_{j=1}a_{ij} = 1
\end{align}
$a_{ij}$称为状态从$s_j$转移到$s_i$的归一化概率（满足式2.x和2.y，下同)。
与其类似，$\Matrix{B}$中元素是隐藏状态$s_j$到时刻$t$的输出结果$O_t$的归一化概率分布，即发射概率：
\begin{equation}
    b_j{k} = P(O_t = o_k |q_t = s_j), 1\leq j \leq M; 1\leq k\leq N
\end{equation}
隐藏状态的初始概率分布$\Matrix{\pi_i}$同样为初始状态的归一化概率分布，定义为：
\begin{equation}
    \pi_i = P(q_1 = s_i), 1\leq i \leq N
\end{equation}
由上述定义可知，隐马尔可夫模型中隐藏状态序列由$\Matrix{\pi}$和$\Matrix{A}$决定，输出的结果，即观察序列由$\Matrix{B}$决定。

对于命名实体识别问题，我们可以将语料中可以观测到的字词理解为观察序列，其所对应的标签序列（是否是命名实体）为隐藏状态序列。
则模型的训练过程为，给定观察序列，即以字或词为序列元素的训练语料来调节模型参数$\Matrix{\pi}$、$\Matrix{A}$、$\Matrix{B}$，使得序列在该模型参数下的概率最大。
模型参数可使用基于Baulm-Welch算法的前后向算法进行训练。

模型的预测过程为，给定模型$\mu = (\Matrix{A},\Matrix{B},\Matrix{\pi}$和一个观察序列$O$，求可能的状态序列$\hat{Q}$，使得
\begin{equation}
    \hat{Q} = \mathop{\arg \max}_Q P(Q|O,\mu)
\end{equation}
预测过程的序列选择结果，可以由Viterbi算法得出。

\subsection{最大熵马尔可夫模型}

（介绍基本原理，选取一篇论文介绍该方法原理在实际的命名实体识别中如何应用）
张华平等人实现的ICTCLAS采用三层层叠隐马尔可夫模型进行命名实体识别。
该方法针对人名、地名和组织机构名分别建立了三层隐马尔可夫模型进行三种命名实体的识别，并分别建立了人名、地名和组织机构名的角色表，即每层隐马尔可夫模型的隐藏状态集合。
该方法首先对原始语料进行切分，得到以词性为标签集的切分语料库；
接着在切分语料库的基础上，以人名角色表为标签集标注语料库，得到人名角色词典，并训练人名角色的转移概率，即识别人名的隐马尔可夫模型参数；
随后根据人名词典，得到识别人名之后的语料库，再以地名角色表为标签集标注语料库，得到地名角色词典，并训练识别地名的隐马尔可夫模型参数；
最后在识别了人名、地名的语料库上以组织机构名为标签集标注语料库，得到组织机构名词典，并训练最后一层隐马尔可夫模型参数。
基于层叠隐马尔可夫模型角色标注的命名实体识别方法，在《人民日报》语料集上的开放测试中，人名、地名和组织机构名识别的F1值分别达到了84.25\%、85.64\%、73.78\%。

与n元语言模型的情况类似，HMM也存在自身的局限性。
首先，马尔可夫性要求，模型某一时刻的隐藏状态只与其前一时刻的状态相关，这一基本假设限制了模型既不能考虑序列后文的信息，也不能考虑距离较远的前文信息;
其次，上述基本假设下的模型也很难完整地抽象自然语言的各类统计特征，不论是词汇还是句法，都较该假设下的模型复杂得多。
最后，单纯依靠对语料建模得到的统计概率进行命名实体识别，是带有较强的领域特征的，模型的可移植性受到了较大的影响。
因此，尽管基于HMM的命名实体识别方法已经得到了广泛的应用，但其很大程度上还是依赖词典来保证识别的效果。

\section{支持向量机与决策树}
\subsection{支持向量机}
（介绍基本原理，选取一篇论文介绍该方法原理在实际的命名实体识别中如何应用）
\subsection{决策树}
（介绍基本原理，选取一篇论文介绍该方法原理在实际的命名实体识别中如何应用）


\section{条件随机场}
（介绍基本原理，选取一篇论文介绍该方法原理在实际的命名实体识别中如何应用）
\section{基于统计语言模型方法的比较和分析}
\section{本章小结}

