\chapter{基于LSTM-CRF的中文序列标注模型}
\section{引言}
由于基于统计的方法在总是在一定的假设前提下对自然语言进行建模，其复杂程度远远不能与灵活多变的自然语言相比，且大多数基于统计的模型若想取得较好的识别效果，总要较大程度地依赖人工构建语料库、字典、特征等手段，且不同语言、不同领域语料之间模型的可迁移性也较差，这在一定程度上限制了基于统计的模型在自然语言处理中进一步的应用。

早在上世纪80年代，神经网络就作为机器学习领域的一个重要的研究方向受到关注。
人工神经网络可以定义为”由具有适应性的简单单元组成的广泛并行互联的网络，它的组织能够模拟生物神经系统对真实世界的物体作出的交互反应“。
其中，这个”具有适应性的简单单元“来源于1943年McCulloch和Pitts提出的M-P神经元模型。
这一模型实际上就是对生物学意义上的神经元进行的一种形式化的数学描述。
早期的神经网络结构较为简单，受到硬件计算能力的限制，一般层数较少，多用于简单模型中；由于结构复杂的网络在训练中存在严重的过拟合问题，且训练效率很低，导致神经网络的发展陷入低谷。
Hinton在2006年提出深度学习的概念，指出深层神经网络可以通过组合低层特征获得抽象的高层特征，这些特征能够更好地对数据的信息进行整合抽象。
同时还提出了基于深度置信网络的非监督贪心逐层训练算法，能够较好地适应深层网络结构，解决了当网络层数较多时的训练难的问题。

深度学习方法在多个领域都有着重要的应用。
Lecun等人提出的卷积神经网络（convolutional neural network，CNN)在图像识别领域得到了广泛应用。
A.Krizhevsky和Hinton等人在2012年将CNN应用于ImageNet数据集上，取得了图像分类和目标定位的最佳结果。
此后CNN在各项任务中都大显身手，在人脸识别、视频分类、行为识别等方向上都取得了较好的效果。
而对于以序列信号作为输入的问题，如语音识别、机器翻译和序列标注等方向，循环神经网络（recurrent neural network，RNN）则成为了解决这类问题的主要手段。
这其中又以能够较好地获取较长距离内上下文依赖的长短时记忆网络(long short-time memory, LSTM）最为突出，

本章针对命名实体识别问题，从几个方面对实现实体识别的基于LSTM-CRF的中文序列标注框架进行介绍。
本章将对词的分布式表示进行介绍，考察循环神经网络的基本结构，对比分析长短时记忆网络及其各种变体的实现，最后在一个具体的命名实体识别任务场景下，讨论基于LSTM-CRF的序列标注框架，并给出针对该任务基于命名实体的额外字符级特征的改进方法，最后设计实验验证改进方法的有效性。

\section{词的分布式表示}
\subsection{从统计语言模型到神经网络语言模型}
不论是传统的基于统计的方法还是基于神经网络的方法，进行命名实体识别任务首先要将自然语言词汇转化为模型能够处理的形式，即实值向量。
最基本的词的向量化表示就是One-hot Representation，其形式为一个维度为词表大小的$\{0, 1\}$向量，设该词在词表中的索引为$i$，则其one-hot向量只在位置$i$上的值为1，其余位置为0。
这种表示方法有很多缺陷，首先不同的词并不能从其对应的one-hot向量中判别其语义相关程度，它基本不包含词汇的语义信息；其次，获取one-hot向量的方式仅仅是通过建立词表即可，忽略了词汇的上下文特征而完全将词独立对待；最后，这种向量形式显然非常稀疏，在深度神经网络模型中非常容易导致维数灾难。

词的分布式表示最初由Hinton于1986年提出。
与one-hot Representation不同的是，分布式表示下的词向量维度不再是词表大小，而是一个相对较低的固定维度。其中的每一个分量也不再是0或者1，而是一个实值。
这时词汇表可以视作是一个向量空间，词即是空间中的点。
既然是空间，就可以定义距离，这个距离可以理解为词汇在语义或语法层面上的距离。
如图\ref{fig:word_embedding}所示，这样词与词之间就不是孤立的了，词在上下文中的语义信息也得到了保留，词向量维度人为可控且远远小于词表大小。

\begin{figure}[!htpb]
    \centering
    \includegraphics[width=\textwidth]{word_embedding.png}
    \caption{词嵌入将词映射到高维空间中的点}
    \label{fig:word_embedding}
\end{figure}

估计词向量参数的方法有很多种，包括隐含语义分析（latent semantic analysis, LSA)、隐狄利克雷分布（latent Dirichlet allocation)和利用神经网络的方法。

Bengio等人在2003年提出了基于神经概率的语言模型。利用神经网络估计语言模型参数的同时，得到了作为副产品的词向量。
其基本的思路是，首先赋予每个词一个词特征向量$\Vector{v}(\Vector{v}\in \Set{R}^m)$，$m$为可以设定的词向量维度，如50，100，200等；然后使用这些词特征向量来表示一段文本序列的联合概率函数：
最后在训练过程中，同时更新（学习）词特征向量和概率函数的参数（语言模型参数）。

在章节2.1.1中介绍了n元语言模型。语言模型的目标是计算给定序列在该语言下的概率，这个概率通过一系列条件概率的乘积表示：
\begin{equation}
    P(w^T_1) = \prod^{T}_{t=1}P(w_t|w_1^{t-1})
\end{equation}
其中$T$为序列长度，$w_i^j = (w_i, w_{i+1},\dots, w_j)$。但由于这种计算方式参数过多，后来模型可以简化为序列某时刻词的条件概率只依赖其上文的若干个词（n-gram）：
\begin{equation}
    P(w_t|w_1^{t-1}) \approx P(w_t|w^{t-1}_{t-n+1})
\end{equation}
通过统计各词序列出现的次数作为条件概率值，填充训练模型参数。

另一种估计模型参数的方法是对模型建立一个目标函数，将参数训练转化为对一个关于模型参数和训练数据的函数优化问题。
实际中，常使用最大对数似然函数作为目标函数，通过调整其参数获得对数似然函数的最大值，此时得到的就是较为理想的参数。
\begin{equation}
    \mathcal{L} = \sum_{c\in C} \log p(w|Context(w))
\end{equation}
在语言模型中，关于某时刻词$w$上下文的条件概率可以用$Context(w)$表示。于是其中的条件概率就可以表示为关于词和其上下文的$Context(w)$的函数：
\begin{equation}
    p(w|Context(w)) = F(w, Context(w), \theta)
\end{equation}
其中$\theta$就是待训练参数集。

在神经语言模型中，获取关于词以其上文为条件的条件概率
\begin{equation}
    f(w_t, \dots, w_{t-n+1}) = P(w_t|w_1^{t-1})
\end{equation}
被分解为两部分，其一是将词表$V$中的词映射为实值向量的矩阵$\Matrix{C}$，其维度是$|V|\times m$；
其二是获得条件概率的映射函数$g$，该函数接受序列$w_{t-n+1}^{t-1}$在实值矩阵$C$中的对应各行向量作为输入，输出给定序列下$w_t$是词典中某个词的概率分布。
于是上面的函数$f$可以进一步表示为：
\begin{equation}
    f(i, w_{t-1}, \dots, w_{t-n+1}) = g(i, C(w_{t-1}), \dots, C(w_{t-n+1}))
\end{equation}

在具体的操作上，模型采用了一个三层神经网络进行参数学习，结构包括输入层、隐藏层和输出层。
值得注意的是其输入层进行的操作是将词$w$上文$n-1$个词的词向量首尾相连得到特征向量$x_w$输入到隐层中。
网络使用传统的前后向传播方法进行参数学习，与其他模型不同的是，在后向传播的最后一步，通过计算$\frac{\partial\mathcal{L}}{\partial x}$并取其中相应维度的部分，实现了一次训练之后批量更新作为输入的$n-1$个词向量的过程。

\subsection{Word2Vector基本原理}
在Bengio工作的基础上，谷歌的Tomas Mikolov团队实现了开源工具word2vec，其中使用了两个模型：CBOW（continous bag of word)模型和Skip-gram模型。

与之前提到的根据上文预测下一个词不同的是，CBOW通过设置一个固定大小的上下文窗口$c$，如设置为2，利用$w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2}$预测当前词$w_t$，Skip-gram则是通过当前词$w_t$预测其上下文。其结构如图\ref{fig:w2v}所示。
神经网络模型将上文词向量连接作为隐层输入，隐层中的激活函数为$\tanh$。
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=\textwidth]{w2v.png}
    \caption{CBOW与Skip-gram模型示意图}
    \label{fig:w2v}
\end{figure}
CBOW同样使用三层的神经网络结构，包括输入层、投影层和输出层，但投影层的输出是通过窗口内$2c$个词向量的和构建的。
并且CBOW模型中并不存在隐藏层，也就没有激活函数。
最后，CBOW对输出层进行了较大的改造：通过Huffman树来构造输出层，将求解词关于其上下文的条件概率的问题转化为二分类问题。
具体来说，输出层构建了一颗包含词典中所有词、并且以这些词为叶子结点的Huffman树。则对于词典中任意一个词，有且仅有一条从根结点出发的路径到达该叶子结点。
从根结点出发，到达每一个非叶子结点时，都会发生一次二分类。图\ref{fig:hierarchical_softmax}展示了这个过程。
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=\textwidth]{hierarchical_softmax.png}
    \caption{Hierarchical softmax模型}
    \label{fig:hierarchical_softmax}
\end{figure}
假设给定分类输入$\Vector{x_w}$，对任意一次二分类，分类到右分支称为正类，记作$d^w_i=1$，分类到左分支称为负类，记作$d^w_i=0$，则分类概率分别可定义为：
\begin{align}
    p_{right} &= \sigma(x_w^\mathrm{T}\theta) = \dfrac{1}{1+e^{-x^\mathrm{T}\theta}}
    p_{left} &= 1 - p_{right}
\end{align}
其中$\theta$就是需要训练的参数。于是，对于一个给定的词$w$，确定其关于上下文的条件概率的过程，就是根据该词的Huffman编码，得到其路径上每一次二分类分到该类$d^w_i$的概率，并将这些概率乘积：
\begin{equation}
    p(w|Context{w}) = \prod^{l(w)}_{j=2}p(d^w_j|\Vector{x_w}, \theta^w_{j-1})
\end{equation}
其中$l(w)$是$w$所在结点的深度。最后通过构建目标函数
\begin{equation}
    \mathcal{L}(w,j) = (1-d^w_j)\log[\sigma(\Vector{X_w}^\mathrm{T}\theta^w_{j-1})] + d^w_j\log[1-\sigma (\Vector{x_w^\mathrm{T}}\theta^w_{j-1})]
\end{equation}
并调整参数使函数值最大化来训练模型。

当确定了$\theta$之后，模型也就确定了。同时在每一次更新参数的过程中，词$w$上下文窗口内的词向量也得到了更新：
\begin{equation}
    \Vector{v(\tilde{w})} := \Vector{v(\tilde{w})} + \eta\sum^{l{w}}_{j=2}\dfrac{\partial\mathcal{L}(w,j)}{\partial \Vector(x_w)}
\end{equation}
其中$\eta$为$w$对上下文词向量贡献的权重系数。

与CBOW相对的Skip-gram模型则在训练过程上十分类似，只是其模型投影层不再有加和等操作，而只把输入层的结果直接用来构造Huffman树。最后训练的过程则是求$p(Context(w)|w)$。具体就不再赘述。

上述过程称为基于Hierarchical Softmax的模型。为了简化训练过程，提高效率并且改善词向量的质量，Mikolov等人又采用Negative Sampling来对模型进行优化，避免了构建较为复杂的Huffman树。
以CBOW模型为例，其思想是指定一个关于当前词$w$的负样本集$NEG(w)$，采取下面的定义来获得词的正负类别标签，用来取代通过Huffam树获得的分类标签：
\begin{equation}
    L^w(\tilde{w}) = \left\{
        \begin{aligned}
            1, &\tilde{w} = w;
            0, &\tilde{w} \neq w;
        \end{aligned}
    \right.
\end{equation}
则给定训练样本$(Context(x), x)$，通过最大化
\begin{equation}
    g(w) = \prod_{u\in\{w\}\cup NEG{w}}p(u|Context(w))
\end{equation}
推导得到目标函数
\begin{equation}
    \mathcal{L} = \sum_{w\in C}\{\log[\sigma(\Vector{x_w}^\mathrm{T}\theta^w)] + \sum_{u\in NEG(w)}\log[\sigma(-\Vector{x_w}^\mathrm{T}\theta^u)\}
\end{equation}
用来训练模型参数。

\section{循环神经网络与长短时记忆网络}
\subsection{循环神经网络}
一般的的神经网络模型中，就其数据流向而言，输入数据是单向传播的，即从输入层到输出层；就其网络结构而言，不同层之间的结点相互连接，但层内结点不存在连接。
这种网络结构在训练时独立看待每一次输入，其输出也是相互独立的；如果输入之间存在相互存在依赖或明显的相关关系，这些信息就很难学习得到。
但实际上，对于以序列形式存在的数据而言，如文本、语音等，这种情况是普遍存在的。

循环神经网络就是针对这类数据而提出的模型，它对序列化的数据有很好的适应性。
RNN的典型特征是，隐藏层中的神经单元存在到自身的连接，即在时域上展开之后，某时刻的结点也与其在前一时刻和后一时刻的状态相连。这样的结构使得模型能够在一定程度上记忆之前输入的信息，即序列中较早时刻输入的元素，也会对模型较晚时刻的输出产生影响。

\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.8\textwidth]{RNN.pdf}
    \caption{RNN基本结构}
    \label{fig:RNN}
\end{figure}

一个基本的RNN也可以看作包含输入层、单层隐藏层和输出层。
对于给定的输入序列$\Tensor{X}=\Vector{x}^n_0$，在每一个时刻，模型的输入是$x_t(x_t \in [0, 1, 2, \dots, n])$。隐层单元将按下式计算当前时刻的隐层状态和输出结果：
\begin{align}
    s_t = f(Ux_t + Ws_{t-1})
    o_t = g(Vs_t)
\end{align}
其中$s_t$是$t$时刻的隐层状态，即模型对序列信息保留的记忆；$U$、$W$、$V$分别是输入层与隐层、隐层单元前后时刻和隐层与输出层之间的连接权重参数。
$f$、$g$是激活函数。

需要说明的是，一般而言我们在讨论RNN时提及的“带有单隐层的RNN”实际上指的是单层RNN单元在时域上展开，因此网络的深度与序列长度有关。
在这个过程中，$U$、$W$、$V$的权重是共享的，即通过整个序列进行训练。
训练RNN的过程可转化为训练多层共享权重的前馈神经网络。
这里的“多层”指的就是RNN隐层在时域上展开后得到的深层神经网络。
训练时，正向计算目标函数后，反向传播误差时计算每一时刻上的误差，最后根据所有时刻上的导数更新权重。
这种算法称为Backpropagation through time, BPTT。

但是在计算梯度时，由于序列长度可能很长，展开后的RNN层数可能很深，这样在计算时域上靠前的层的梯度时，就会产生多个导数相乘的形式，若其中存在多个值较小，那么其相乘的结果会迅速收敛到0，这就是“梯度消失”问题。

RNN普遍存在这种问题，从结果上看，这会导致RNN难以处理前后相隔较远的上下文依赖。一个简单的解决方案是替换激活函数，使用ReLu替换sigmoid或$\tanh$。而在RNN的基础上修改其单元的内部实现，比如使用长短时记忆模型（LSTM），则是更有效更普遍的方法。

\subsection{长短时记忆网络及其变体}
上文已经提及，虽然除$t_0$时刻的RNN单元外后续的每个单元接收的输入都带有之前所有时刻信息的累积，但受限于梯度消失问题，相隔较远的时刻输入对参数学习将不会有贡献，导致时域上靠后的单元无法很好地感知靠前的单元，句子内的长程依赖信息就无法被学习。

针对这一问题，S.Hochreiter和J.Schmidhuber在1997年提出了LSTM，其本质是使用带有信息存储和更新机制的单元代替RNN中相对较为简单的激活单元。信息存储即cell的内部状态，而更新机制则通过一系列“门控单元”实现，包括输入门、遗忘门和输出门。这些“门”会根据参数，对输入、cell状态和输出进行一系列处理，主要是对cell状态的更新进行控制，决定在状态上添加信息（记忆）还是减少信息（遗忘）。

\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.8\textwidth]{LSTM-CIFG}
    \caption{LSTM的一般结构}
    \label{fig:LSTM}
\end{figure}

一个基本的LSTM实现如图\ref{fig:LSTM}所示。其内部结构可描述为：
\begin{align}
    \Vector{i_t} &= \sigma(\Matrix{W_i}\Vector{x_t} + \Matrix{U_i}\Vector{h_{t-1}} + \Vector{b_i})
    \label{eq:lstm_i}\\
    \Vector{f_t} &= \sigma(\Matrix{W_f}\Vector{x_t} + \Matrix{U_f}\Vector{h_{t-1}} + \Vector{b_f})\\
    \Vector{o_t} &= \sigma(\Matrix{W_o}\Vector{x_t} + \Matrix{U_o}\Vector{h_{t-1}} + \Vector{b_o})\\
    \Vector{a_t} &= \tanh(\Matrix{W_C}\Vector{x_t} + \Matrix{U_C}\Vector{h_{t-1}} + \Vector{b_C})\\
    \Vector{C_t} &= \Vector{f_t}\odot\Vector{C_{t-1}} +
                    \Vector{i_t}\odot\Vector{a}_t
                    \label{eq:cell state}\\
    \Vector{h_t} &= \Vector{o_t} \odot \tanh(\Vector{C_t})\label{eq:lstm_C}
\end{align}
其中$\odot$代表元素乘积，其结果仍是一个固定维度的实向量。

首先，上一时刻的隐藏层状态$\Vector{h_{t-1}}$和当前时刻的输入词$\Vector{x_t}$经过连接构成了当前时刻的输入信息向量。这一向量将作为所有门控层$sigmoid$激活函数的参数。

而经$\tanh$激活函数获得的激活值$\Vector{a}_t$则是一个临时的状态向量，其中包括了部分应该保留在cell状态中的信息，而其他信息则应该丢弃。

接着门控层根据模型参数和输入向量获得遗忘参数$\Vector{f}_t$和输入参数$\Vector{i}_t$。由式\ref{eq:cell state}可知，当前的cell状态将受到在输入门$\Vector{i}_t$控制下的临时状态向量$\Vector{a}_t$和在遗忘门$\Vector{f}_t$控之下的前一时刻cell状态$\Vector{C}_{t-1}$的共同影响。它们综合决定临时状态与历史状态中哪些信息应该得到保留，哪些信息应该丢弃。最终两部分保留的结果相加确定当前的cell状态。

为了更直观地观察到参数关系与维度，我们引入文献[]给出的形式来表示模型。在式\ref{eq:lstm_i}到\ref{eq:lstm_C}的基础上，设偏置向量$b*$整合进了$x_t$且:
\begin{align}
    \hat{\Vector{i}}_t &= \Matrix{W_i}\Vector{x_t} + \Matrix{U_i}\Vector{h_{t-1}}\\
    \hat{\Vector{f}}_t &= \Matrix{W_f}\Vector{x_t} + \Matrix{U_f}\Vector{h_{t-1}}\\
    \hat{\Vector{o}}_t &= \Matrix{W_o}\Vector{x_t} + \Matrix{U_o}\Vector{h_{t-1}}\\
    \hat{\Vector{a}}_t &= \Matrix{W_C}\Vector{x_t} + \Matrix{U_C}\Vector{h_{t-1}}
\end{align}
则省略激活函数的情况下，参数可以组织为：
\begin{equation}
    \Vector{z}_t =
    \begin{bmatrix}
        \hat{\Vector{a}}_t\\
        \hat{\Vector{i}}_t\\
        \hat{\Vector{f}}_t\\
        \hat{\Vector{o}}_t\\
    \end{bmatrix}
    =
    \begin{bmatrix}
        \Matrix{W}_C & \Matrix{U}_C\\
        \Matrix{W}_i & \Matrix{U}_i\\
        \Matrix{W}_f & \Matrix{U}_f\\
        \Matrix{W}_o & \Matrix{U}_o\\
    \end{bmatrix}
    \times
    \begin{bmatrix}
        \Vector{x}_t\\
        \Vector{h}_{t-1}
    \end{bmatrix}
    =
    \begin{bmatrix}
        \Matrix{W}\times\Matrix{I}_t
    \end{bmatrix}
\end{equation}
若$x_t$为$n$维向量，隐层单元数为$m$，则$\hat{\Vector{a}}_t$，$\hat{\Vector{i}}_t$，$\hat{\Vector{f}}_t$，$\hat{\Vector{o}}_t$对应的权重矩阵$W*$、$U*$维数分别为$m\times n$、$m\times m$。
于是$W$的维数即为$4m\times(m+n)$。

上述基本的LSTM实现中，cell的历史状态$C_{t-1}$并不能影响遗忘门$\Vector{f}_t$和输入门$\Vector{i}_t$对临时状态$\Vector{a}$和自身信息是否保留的决策。
因此文献[]给出了LSTM cell的一个改进版本，它在原有LSTM定义的输入向量上扩展了历史cell状态的部分，即在进行遗忘/保留决策时，还需要考虑到历史记忆。
这一机制称为peep-hole，即在图\ref{fig:LSTM}所示LSTM结构基础上增加虚线部分。
其内部参数可表示为：
\begin{align}
    \Vector{i}_t &= \sigma(\Matrix{W_{i}}\Vector{x_{t}} + \Matrix{W_{i}}\Vector{h_{t-1}} + \Matrix{W_{i}}\Vector{C_{t-1}} + \Vector{b_i})\\
    \Vector{f}_t &= \sigma(\Matrix{W_{f}}\Vector{x_{t}} + \Matrix{W_{f}}\Vector{h_{t-1}} + \Matrix{W_{f}}\Vector{C_{t-1}} + \Vector{b_f})\\
    \Vector{o}_t &= \sigma(\Matrix{W_{o}}\Vector{x_{t}} + \Matrix{W_{o}}\Vector{h_{t-1}} + \Matrix{W_{o}}\Vector{C_{t-1}} + \Vector{b_o})\\
    \Vector{a}_t &= \tanh(\Matrix{W_{C}}\Vector{x_t} + \Matrix{W_{C}}\Vector{h_{t-1}} + \Vector{b_{C}})\\
    \Vector{C}_t &= \Vector{f_t}\odot\Vector{C_{t-1}} + \Vector{i_t}\odot \Vector{a}_t \\
    \Vector{h}_t &= \Vector{o_t} \odot \tanh(\Vector{C_t})
\end{align}
在上述使用peep-hole的LSTM cell的基础上，文献[]在命名实体识别任务上还使用了耦合输入-遗忘门（Coupled Input-Forget Gate, CIFG）[]取得了较好的效果，即令
\begin{equation}
    \Vector{f_t} = 1 - \Vector{i_t}
\end{equation}
对比LSTM cell简化了CIFG的计算:
\begin{equation}
    \Vector{C_t} = (1 - \Vector{i_t})\odot \Vector{C_{t-1}} + \Vector{i_t}\odot \tanh(\Matrix{W_{Cx}}\Vector{x_t} + \Matrix{W_{ch}}\Vector{h_{t-1}} + \Vector{b_C})
\end{equation}

最后，文献[]提出了一种更为简单的LSTM单元的变体，称为门控循环单元(Gated Recurrent Unit，GRU)。
GRU存在多种变体，区别在于计算每个门层输出是否使用之前的隐藏状态和偏置。
本节仅介绍使用隐藏状态计算门层输出的GRU，如图\ref{fig:GRU}所示。
其最主要的特征是
\begin{enumerate}
    \item 融合了cell状态和隐藏层输出
    \item 融合了输入门和遗忘门
\end{enumerate}

\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.8\linewidth]{GRU}
    \caption{GRU基本结构}
    \label{fig:GRU}
\end{figure}
这使得单元相比LSTM更加简单，参数更少，同时实验表明它能够在大部分任务中取得与LSTM相当或更佳的效果。
其内部参数可表示为：
\begin{align}
    \Vector{z_t} &= \sigma(\Matrix{W_{z}}\Vector{x_t} + \Matrix{W_{z}}\Vector{h_{t-1}})\\
    \Vector{r_t} &= \sigma(\Matrix{W_{r}}\Vector{x_t} + \Matrix{W_{r}}\Vector{h_{t-1}})\\
    \Vector{\tilde{h_t}} &= \tanh(\Matrix{W_{\tilde{h}u}}(\Vector{r_t} \odot \Vector{h_{t-1}}) + \Matrix{W_{\tilde{h}x}}\Vector{x_t})\\
    \Vector{h_t} &= (1 - \Vector{z_t}) \odot \Vector{h_{t-1}} + \Vector{z_t} \odot \Vector{\tilde{h_t}}
\end{align}

LSTM存在很多种变体，也有文献系统地对比了多种变体在自然语言处理任务上的性能。本文也将在具体的场景下，对比分析不同的结构对命名实体识别效果的影响。

\section{双向LSTM-CRF序列标注框架}
\subsection{双向LSTM层}
\subsection{CRF层}
\section{模型训练与参数调优}
\subsection{批量训练}
\subsection{防止过拟合}
\section{本章小结}

